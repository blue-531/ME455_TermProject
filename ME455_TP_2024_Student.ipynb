{"cells":[{"cell_type":"markdown","metadata":{"id":"MS39g1O3mOKW"},"source":["# [2024 ME455 Term Project]\n","# Collision Risk Index estimation with Semantic segmentation, Mono Depth and Object detection"]},{"cell_type":"markdown","metadata":{"id":"8IIw3k0H7kWb"},"source":["\n","#Semantic Segmentation"]},{"cell_type":"markdown","metadata":{"id":"uaJ31kAcottW"},"source":["Define normalization, upsample etc."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":2564,"status":"ok","timestamp":1669880470435,"user":{"displayName":"윤성훈","userId":"03235101598759790024"},"user_tz":-540},"id":"dX8I8pgdhXL8"},"outputs":[],"source":["#@title\n","\"\"\"\n","This code is from https://github.com/NVIDIA/semantic-segmentation\n","\"\"\"\n","\n","\n","\"\"\"\n","Custom Norm wrappers to enable sync BN, regular BN and for weight initialization\n","\"\"\"\n","import torch.nn as nn\n","import numpy as np\n","from torchvision import transforms\n","\n","# from config import cfg\n","\n","# from apex import amp\n","\n","\n","def Norm2d(in_channels):\n","    \"\"\"\n","    Custom Norm Function to allow flexible switching\n","    \"\"\"\n","    layer = torch.nn.BatchNorm2d #getattr(cfg.MODEL, 'BNFUNC')\n","    normalization_layer = layer(in_channels)\n","    return normalization_layer\n","\n","\n","def initialize_weights(*models):\n","    \"\"\"\n","    Initialize Model Weights\n","    \"\"\"\n","    for model in models:\n","        for module in model.modules():\n","            if isinstance(module, (nn.Conv2d, nn.Linear)):\n","                nn.init.kaiming_normal_(module.weight)\n","                if module.bias is not None:\n","                    module.bias.data.zero_()\n","            elif isinstance(module, nn.BatchNorm2d):\n","                module.weight.data.fill_(1)\n","                module.bias.data.zero_()\n","\n","\n","# @amp.float_function\n","def Upsample(x, size):\n","    \"\"\"\n","    Wrapper Around the Upsample Call\n","    \"\"\"\n","    return nn.functional.interpolate(x, size=size, mode='bilinear',\n","                                     align_corners=True)\n","\n","\n","def decode_segmap(temp):\n","    colors = [  # [  0,   0,   0],\n","        [128, 64, 128],#road\n","        [244, 35, 232],#sidewalk\n","        [70, 70, 70],#building\n","        [102, 102, 156],#wall\n","        [190, 153, 153],#fence\n","        [153, 153, 153],#pole\n","        [250, 170, 30],#traffic light\n","        [220, 220, 0],#trafiic sign\n","        [107, 142, 35],  # vegetation dark green\n","        [152, 251, 152],  # terrain bright green\n","        [0, 130, 180],#sky\n","        [220, 20, 60],\n","        [255, 0, 0],\n","        [0, 0, 142],\n","        [0, 0, 70],\n","        [0, 60, 100],\n","        [0, 80, 100],\n","        [0, 0, 230],\n","        [119, 11, 32],\n","    ]\n","\n","    label_colours = dict(zip(range(19), colors))\n","    r = temp.copy()\n","    g = temp.copy()\n","    b = temp.copy()\n","    for l in range(0, 19):\n","        r[temp == l] = label_colours[l][0]\n","        g[temp == l] = label_colours[l][1]\n","        b[temp == l] = label_colours[l][2]\n","\n","    rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n","    rgb[:, :, 0] = r #/ 255.0\n","    rgb[:, :, 1] = g #/ 255.0\n","    rgb[:, :, 2] = b #/ 255.0\n","    return rgb\n","\n","\n","def denorm(img):\n","    # ImageNet statistics\n","    mean_img = [0.485, 0.456, 0.406]\n","    std_img = [0.229, 0.224, 0.225]\n","\n","    tf_denorm = transforms.Normalize(mean = [-mean_img[0] / std_img[0], -mean_img[1] / std_img[1], -mean_img[2] / std_img[2]],\n","                                     std = [1 / std_img[0], 1 / std_img[1], 1 / std_img[2]])\n","\n","    return tf_denorm(img)\n"]},{"cell_type":"markdown","metadata":{"id":"UL9aY5YGpKww"},"source":["Define resblock and WIDE resnet"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"UtxDFJjBo4k6"},"outputs":[],"source":["\n","\"\"\"\n","# Code adapted from:\n","# https://github.com/mapillary/inplace_abn/\n","#\n","# BSD 3-Clause License\n","#\n","# Copyright (c) 2017, mapillary\n","# All rights reserved.\n","#\n","# Redistribution and use in source and binary forms, with or without\n","# modification, are permitted provided that the following conditions are met:\n","#\n","# * Redistributions of source code must retain the above copyright notice, this\n","#   list of conditions and the following disclaimer.\n","#\n","# * Redistributions in binary form must reproduce the above copyright notice,\n","#   this list of conditions and the following disclaimer in the documentation\n","#   and/or other materials provided with the distribution.\n","#\n","# * Neither the name of the copyright holder nor the names of its\n","#   contributors may be used to endorse or promote products derived from\n","#   this software without specific prior written permission.\n","#\n","# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n","# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n","# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n","# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n","# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n","# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n","# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n","# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n","# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n","# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","\"\"\"\n","import logging\n","import sys\n","from collections import OrderedDict\n","from functools import partial\n","import torch.nn as nn\n","import torch\n","# import network.mynn as mynn\n","\n","def bnrelu(channels):\n","    \"\"\"\n","    Single Layer BN and Relui\n","    \"\"\"\n","    return nn.Sequential(Norm2d(channels),\n","                         nn.ReLU(inplace=True))\n","\n","class GlobalAvgPool2d(nn.Module):\n","    \"\"\"\n","    Global average pooling over the input's spatial dimensions\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(GlobalAvgPool2d, self).__init__()\n","        logging.info(\"Global Average Pooling Initialized\")\n","\n","    def forward(self, inputs):\n","        in_size = inputs.size()\n","        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n","\n","\n","class IdentityResidualBlock(nn.Module):\n","    \"\"\"\n","    Identity Residual Block for WideResnet\n","    \"\"\"\n","    def __init__(self,\n","                 in_channels,\n","                 channels,\n","                 stride=1,\n","                 dilation=1,\n","                 groups=1,\n","                 norm_act=bnrelu,\n","                 dropout=None,\n","                 dist_bn=False\n","                 ):\n","        \"\"\"Configurable identity-mapping residual block\n","\n","        Parameters\n","        ----------\n","        in_channels : int\n","            Number of input channels.\n","        channels : list of int\n","            Number of channels in the internal feature maps.\n","            Can either have two or three elements: if three construct\n","            a residual block with two `3 x 3` convolutions,\n","            otherwise construct a bottleneck block with `1 x 1`, then\n","            `3 x 3` then `1 x 1` convolutions.\n","        stride : int\n","            Stride of the first `3 x 3` convolution\n","        dilation : int\n","            Dilation to apply to the `3 x 3` convolutions.\n","        groups : int\n","            Number of convolution groups.\n","            This is used to create ResNeXt-style blocks and is only compatible with\n","            bottleneck blocks.\n","        norm_act : callable\n","            Function to create normalization / activation Module.\n","        dropout: callable\n","            Function to create Dropout Module.\n","        dist_bn: Boolean\n","            A variable to enable or disable use of distributed BN\n","        \"\"\"\n","        super(IdentityResidualBlock, self).__init__()\n","        self.dist_bn = dist_bn\n","\n","        # Check if we are using distributed BN and use the nn from encoding.nn\n","        # library rather than using standard pytorch.nn\n","\n","\n","        # Check parameters for inconsistencies\n","        if len(channels) != 2 and len(channels) != 3:\n","            raise ValueError(\"channels must contain either two or three values\")\n","        if len(channels) == 2 and groups != 1:\n","            raise ValueError(\"groups > 1 are only valid if len(channels) == 3\")\n","\n","        is_bottleneck = len(channels) == 3\n","        need_proj_conv = stride != 1 or in_channels != channels[-1]\n","\n","        self.bn1 = norm_act(in_channels)\n","        if not is_bottleneck:\n","            layers = [\n","                (\"conv1\", nn.Conv2d(in_channels,\n","                                    channels[0],\n","                                    3,\n","                                    stride=stride,\n","                                    padding=dilation,\n","                                    bias=False,\n","                                    dilation=dilation)),\n","                (\"bn2\", norm_act(channels[0])),\n","                (\"conv2\", nn.Conv2d(channels[0], channels[1],\n","                                    3,\n","                                    stride=1,\n","                                    padding=dilation,\n","                                    bias=False,\n","                                    dilation=dilation))\n","            ]\n","            if dropout is not None:\n","                layers = layers[0:2] + [(\"dropout\", dropout())] + layers[2:]\n","        else:\n","            layers = [\n","                (\"conv1\",\n","                 nn.Conv2d(in_channels,\n","                           channels[0],\n","                           1,\n","                           stride=stride,\n","                           padding=0,\n","                           bias=False)),\n","                (\"bn2\", norm_act(channels[0])),\n","                (\"conv2\", nn.Conv2d(channels[0],\n","                                    channels[1],\n","                                    3, stride=1,\n","                                    padding=dilation, bias=False,\n","                                    groups=groups,\n","                                    dilation=dilation)),\n","                (\"bn3\", norm_act(channels[1])),\n","                (\"conv3\", nn.Conv2d(channels[1], channels[2],\n","                                    1, stride=1, padding=0, bias=False))\n","            ]\n","            if dropout is not None:\n","                layers = layers[0:4] + [(\"dropout\", dropout())] + layers[4:]\n","        self.convs = nn.Sequential(OrderedDict(layers))\n","\n","        if need_proj_conv:\n","            self.proj_conv = nn.Conv2d(\n","                in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        This is the standard forward function for non-distributed batch norm\n","        \"\"\"\n","        if hasattr(self, \"proj_conv\"):\n","            bn1 = self.bn1(x)\n","            shortcut = self.proj_conv(bn1)\n","        else:\n","            shortcut = x.clone()\n","            bn1 = self.bn1(x)\n","\n","        out = self.convs(bn1)\n","        out.add_(shortcut)\n","        return out\n","\n","\n","\n","\n","class WiderResNet(nn.Module):\n","    \"\"\"\n","    WideResnet Global Module for Initialization\n","    \"\"\"\n","    def __init__(self,\n","                 structure,\n","                 norm_act=bnrelu,\n","                 classes=0\n","                 ):\n","        \"\"\"Wider ResNet with pre-activation (identity mapping) blocks\n","\n","        Parameters\n","        ----------\n","        structure : list of int\n","            Number of residual blocks in each of the six modules of the network.\n","        norm_act : callable\n","            Function to create normalization / activation Module.\n","        classes : int\n","            If not `0` also include global average pooling and \\\n","            a fully-connected layer with `classes` outputs at the end\n","            of the network.\n","        \"\"\"\n","        super(WiderResNet, self).__init__()\n","        self.structure = structure\n","\n","        if len(structure) != 6:\n","            raise ValueError(\"Expected a structure with six values\")\n","\n","        # Initial layers\n","        self.mod1 = nn.Sequential(OrderedDict([\n","            (\"conv1\", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n","        ]))\n","\n","        # Groups of residual blocks\n","        in_channels = 64\n","        channels = [(128, 128), (256, 256), (512, 512), (512, 1024),\n","                    (512, 1024, 2048), (1024, 2048, 4096)]\n","        for mod_id, num in enumerate(structure):\n","            # Create blocks for module\n","            blocks = []\n","            for block_id in range(num):\n","                blocks.append((\n","                    \"block%d\" % (block_id + 1),\n","                    IdentityResidualBlock(in_channels, channels[mod_id],\n","                                          norm_act=norm_act)\n","                ))\n","\n","                # Update channels and p_keep\n","                in_channels = channels[mod_id][-1]\n","\n","            # Create module\n","            if mod_id <= 4:\n","                self.add_module(\"pool%d\" %\n","                                (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n","            self.add_module(\"mod%d\" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n","\n","        # Pooling and predictor\n","        self.bn_out = norm_act(in_channels)\n","        if classes != 0:\n","            self.classifier = nn.Sequential(OrderedDict([\n","                (\"avg_pool\", GlobalAvgPool2d()),\n","                (\"fc\", nn.Linear(in_channels, classes))\n","            ]))\n","\n","    def forward(self, img):\n","        out = self.mod1(img)\n","        out = self.mod2(self.pool2(out))\n","        out = self.mod3(self.pool3(out))\n","        out = self.mod4(self.pool4(out))\n","        out = self.mod5(self.pool5(out))\n","        out = self.mod6(self.pool6(out))\n","        out = self.mod7(out)\n","        out = self.bn_out(out)\n","\n","        if hasattr(self, \"classifier\"):\n","            out = self.classifier(out)\n","\n","        return out\n","\n","\n","class WiderResNetA2(nn.Module):\n","    \"\"\"\n","    Wider ResNet with pre-activation (identity mapping) blocks\n","\n","    This variant uses down-sampling by max-pooling in the first two blocks and\n","     by strided convolution in the others.\n","\n","    Parameters\n","    ----------\n","    structure : list of int\n","        Number of residual blocks in each of the six modules of the network.\n","    norm_act : callable\n","        Function to create normalization / activation Module.\n","    classes : int\n","        If not `0` also include global average pooling and a fully-connected layer\n","        with `classes` outputs at the end\n","        of the network.\n","    dilation : bool\n","        If `True` apply dilation to the last three modules and change the\n","        down-sampling factor from 32 to 8.\n","    \"\"\"\n","    def __init__(self,\n","                 structure,\n","                 norm_act=bnrelu,\n","                 classes=0,\n","                 dilation=False,\n","                 dist_bn=False\n","                 ):\n","        super(WiderResNetA2, self).__init__()\n","        self.dist_bn = dist_bn\n","\n","        # If using distributed batch norm, use the encoding.nn as oppose to torch.nn\n","\n","\n","        nn.Dropout = nn.Dropout2d\n","        norm_act = bnrelu\n","        self.structure = structure\n","        self.dilation = dilation\n","\n","        if len(structure) != 6:\n","            raise ValueError(\"Expected a structure with six values\")\n","\n","        # Initial layers\n","        self.mod1 = torch.nn.Sequential(OrderedDict([\n","            (\"conv1\", nn.Conv2d(3, 64, 3, stride=1, padding=1, bias=False))\n","        ]))\n","\n","        # Groups of residual blocks\n","        in_channels = 64\n","        channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n","                    (1024, 2048, 4096)]\n","        for mod_id, num in enumerate(structure):\n","            # Create blocks for module\n","            blocks = []\n","            for block_id in range(num):\n","                if not dilation:\n","                    dil = 1\n","                    stride = 2 if block_id == 0 and 2 <= mod_id <= 4 else 1\n","                else:\n","                    if mod_id == 3:\n","                        dil = 2\n","                    elif mod_id > 3:\n","                        dil = 4\n","                    else:\n","                        dil = 1\n","                    stride = 2 if block_id == 0 and mod_id == 2 else 1\n","\n","                if mod_id == 4:\n","                    drop = partial(nn.Dropout, p=0.3)\n","                elif mod_id == 5:\n","                    drop = partial(nn.Dropout, p=0.5)\n","                else:\n","                    drop = None\n","\n","                blocks.append((\n","                    \"block%d\" % (block_id + 1),\n","                    IdentityResidualBlock(in_channels,\n","                                          channels[mod_id], norm_act=norm_act,\n","                                          stride=stride, dilation=dil,\n","                                          dropout=drop, dist_bn=self.dist_bn)\n","                ))\n","\n","                # Update channels and p_keep\n","                in_channels = channels[mod_id][-1]\n","\n","            # Create module\n","            if mod_id < 2:\n","                self.add_module(\"pool%d\" %\n","                                (mod_id + 2), nn.MaxPool2d(3, stride=2, padding=1))\n","            self.add_module(\"mod%d\" % (mod_id + 2), nn.Sequential(OrderedDict(blocks)))\n","\n","        # Pooling and predictor\n","        self.bn_out = norm_act(in_channels)\n","        if classes != 0:\n","            self.classifier = nn.Sequential(OrderedDict([\n","                (\"avg_pool\", GlobalAvgPool2d()),\n","                (\"fc\", nn.Linear(in_channels, classes))\n","            ]))\n","\n","    def forward(self, img):\n","        out = self.mod1(img)\n","        out = self.mod2(self.pool2(out))\n","        out = self.mod3(self.pool3(out))\n","        out = self.mod4(out)\n","        out = self.mod5(out)\n","        out = self.mod6(out)\n","        out = self.mod7(out)\n","        out = self.bn_out(out)\n","\n","        if hasattr(self, \"classifier\"):\n","            return self.classifier(out)\n","        return out\n","\n","\n","_NETS = {\n","    \"16\": {\"structure\": [1, 1, 1, 1, 1, 1]},\n","    \"20\": {\"structure\": [1, 1, 1, 3, 1, 1]},\n","    \"38\": {\"structure\": [3, 3, 6, 3, 1, 1]},\n","}\n","\n","__all__ = []\n","for name, params in _NETS.items():\n","    net_name = \"wider_resnet\" + name\n","    setattr(sys.modules[__name__], net_name, partial(WiderResNet, **params))\n","    __all__.append(net_name)\n","for name, params in _NETS.items():\n","    net_name = \"wider_resnet\" + name + \"_a2\"\n","    setattr(sys.modules[__name__], net_name, partial(WiderResNetA2, **params))\n","    __all__.append(net_name)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4hPlqdGZpS62"},"source":["Define ASPP and DeeplabV3plus+wideresnet backbone"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9a1Xb5EtpArW"},"outputs":[],"source":["\n","class _AtrousSpatialPyramidPoolingModule(nn.Module):\n","    \"\"\"\n","    operations performed:\n","      1x1 x depth\n","      3x3 x depth dilation 6\n","      3x3 x depth dilation 12\n","      3x3 x depth dilation 18\n","      image pooling\n","      concatenate all together\n","      Final 1x1 conv\n","    \"\"\"\n","\n","    def __init__(self, in_dim, reduction_dim=256, output_stride=16, rates=(6, 12, 18)):\n","        super(_AtrousSpatialPyramidPoolingModule, self).__init__()\n","\n","        # Check if we are using distributed BN and use the nn from encoding.nn\n","        # library rather than using standard pytorch.nn\n","\n","        if output_stride == 8:\n","            rates = [2 * r for r in rates]\n","        elif output_stride == 16:\n","            pass\n","        else:\n","            raise 'output stride of {} not supported'.format(output_stride)\n","\n","        self.features = []\n","        # 1x1\n","        self.features.append(\n","            nn.Sequential(nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n","                          Norm2d(reduction_dim), nn.ReLU(inplace=True)))\n","        # other rates\n","        for r in rates:\n","            self.features.append(nn.Sequential(\n","                nn.Conv2d(in_dim, reduction_dim, kernel_size=3,\n","                          dilation=r, padding=r, bias=False),\n","                Norm2d(reduction_dim),\n","                nn.ReLU(inplace=True)\n","            ))\n","        self.features = torch.nn.ModuleList(self.features)\n","\n","        # img level features\n","        self.img_pooling = nn.AdaptiveAvgPool2d(1)\n","        self.img_conv = nn.Sequential(\n","            nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n","            Norm2d(reduction_dim), nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        x_size = x.size()\n","\n","        img_features = self.img_pooling(x)\n","        img_features = self.img_conv(img_features)\n","        img_features = Upsample(img_features, x_size[2:])\n","        out = img_features\n","\n","        for f in self.features:\n","            y = f(x)\n","            out = torch.cat((out, y), 1)\n","        return out\n","\n","class DeepWV3Plus(nn.Module):\n","    \"\"\"\n","    Wide_resnet version of DeepLabV3\n","    mod1\n","    pool2\n","    mod2 str2\n","    pool3\n","    mod3-7\n","\n","      structure: [3, 3, 6, 3, 1, 1]\n","      channels = [(128, 128), (256, 256), (512, 512), (512, 1024), (512, 1024, 2048),\n","                  (1024, 2048, 4096)]\n","    \"\"\"\n","\n","    def __init__(self, num_classes, trunk='WideResnet38', criterion=None):\n","\n","        super(DeepWV3Plus, self).__init__()\n","        self.criterion = criterion\n","        logging.info(\"Trunk: %s\", trunk)\n","        wide_resnet = wider_resnet38_a2(classes=1000, dilation=True)\n","        # TODO: Should this be even here ?\n","        wide_resnet = torch.nn.DataParallel(wide_resnet)\n","        try:\n","            checkpoint = torch.load('./pretrained_models/wider_resnet38.pth.tar', map_location='cpu')\n","            wide_resnet.load_state_dict(checkpoint['state_dict'])\n","            del checkpoint\n","        except:\n","            print(\"=====================Could not load ImageNet weights=======================\")\n","            print(\"Please download the ImageNet weights of WideResNet38 in our repo to ./pretrained_models.\")\n","\n","        wide_resnet = wide_resnet.module\n","\n","        self.mod1 = wide_resnet.mod1\n","        self.mod2 = wide_resnet.mod2\n","        self.mod3 = wide_resnet.mod3\n","        self.mod4 = wide_resnet.mod4\n","        self.mod5 = wide_resnet.mod5\n","        self.mod6 = wide_resnet.mod6\n","        self.mod7 = wide_resnet.mod7\n","        self.pool2 = wide_resnet.pool2\n","        self.pool3 = wide_resnet.pool3\n","        del wide_resnet\n","\n","        self.aspp = _AtrousSpatialPyramidPoolingModule(4096, 256,\n","                                                       output_stride=8)\n","\n","        self.bot_fine = nn.Conv2d(128, 48, kernel_size=1, bias=False)\n","        self.bot_aspp = nn.Conv2d(1280, 256, kernel_size=1, bias=False)\n","\n","        self.final = nn.Sequential(\n","            nn.Conv2d(256 + 48, 256, kernel_size=3, padding=1, bias=False),\n","            Norm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n","            Norm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, num_classes, kernel_size=1, bias=False))\n","\n","        initialize_weights(self.final)\n","\n","    def forward(self, inp, gts=None):\n","\n","        x_size = inp.size()\n","        x = self.mod1(inp)\n","        m2 = self.mod2(self.pool2(x))\n","        x = self.mod3(self.pool3(m2))\n","        x = self.mod4(x)\n","        x = self.mod5(x)\n","        x = self.mod6(x)\n","        x = self.mod7(x)\n","        x = self.aspp(x)\n","        dec0_up = self.bot_aspp(x)\n","\n","        dec0_fine = self.bot_fine(m2)\n","        dec0_up = Upsample(dec0_up, m2.size()[2:])\n","        dec0 = [dec0_fine, dec0_up]\n","        dec0 = torch.cat(dec0, 1)\n","\n","        dec1 = self.final(dec0)\n","        out = Upsample(dec1, x_size[2:])\n","\n","        if self.training:\n","            return self.criterion(out, gts)\n","\n","        return out"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19905,"status":"ok","timestamp":1669880500140,"user":{"displayName":"윤성훈","userId":"03235101598759790024"},"user_tz":-540},"id":"kqVVtprgpkDh","outputId":"42522c96-05c1-47df-d733-7122bb0e814b"},"outputs":[{"name":"stdout","output_type":"stream","text":["=====================Could not load ImageNet weights=======================\n","Please download the ImageNet weights of WideResNet38 in our repo to ./pretrained_models.\n"]},{"data":{"text/plain":["DataParallel(\n","  (module): DeepWV3Plus(\n","    (mod1): Sequential(\n","      (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","    )\n","    (mod2): Sequential(\n","      (block1): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (proj_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (block2): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (block3): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","    (mod3): Sequential(\n","      (block1): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (proj_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (block2): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (block3): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","    (mod4): Sequential(\n","      (block1): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","        (proj_conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","      )\n","      (block2): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (block3): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (block4): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (block5): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","      (block6): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        )\n","      )\n","    )\n","    (mod5): Sequential(\n","      (block1): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        )\n","        (proj_conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (block2): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        )\n","      )\n","      (block3): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        )\n","      )\n","    )\n","    (mod6): Sequential(\n","      (block1): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","          (bn3): Sequential(\n","            (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (dropout): Dropout2d(p=0.3, inplace=False)\n","          (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        )\n","        (proj_conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","    (mod7): Sequential(\n","      (block1): IdentityResidualBlock(\n","        (bn1): Sequential(\n","          (0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (1): ReLU(inplace=True)\n","        )\n","        (convs): Sequential(\n","          (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn2): Sequential(\n","            (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (conv2): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n","          (bn3): Sequential(\n","            (0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (1): ReLU(inplace=True)\n","          )\n","          (dropout): Dropout2d(p=0.5, inplace=False)\n","          (conv3): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        )\n","        (proj_conv): Conv2d(2048, 4096, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","    )\n","    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (pool3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (aspp): _AtrousSpatialPyramidPoolingModule(\n","      (features): ModuleList(\n","        (0): Sequential(\n","          (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (1): Sequential(\n","          (0): Conv2d(4096, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (2): Sequential(\n","          (0): Conv2d(4096, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (3): Sequential(\n","          (0): Conv2d(4096, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","      )\n","      (img_pooling): AdaptiveAvgPool2d(output_size=1)\n","      (img_conv): Sequential(\n","        (0): Conv2d(4096, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU(inplace=True)\n","      )\n","    )\n","    (bot_fine): Conv2d(128, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (bot_aspp): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (final): Sequential(\n","      (0): Conv2d(304, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU(inplace=True)\n","      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (5): ReLU(inplace=True)\n","      (6): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    )\n","  )\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["net_SS = DeepWV3Plus(19).cuda() # declare Network and the number of class=19\n","state_dict= torch.load('ckpts/segmentation.pth')\n","net_SS = torch.nn.DataParallel(net_SS)\n","    \n","net_SS.load_state_dict(state_dict['state_dict'],strict=False) #load the checkpoint(state_dict) \n","\n","net_SS.eval()\n"]},{"cell_type":"markdown","metadata":{"id":"GvImz6EfnUhI"},"source":["# Mono depth"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"UTizbw_D9e3d"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","import torch.utils.model_zoo as model_zoo\n","from collections import OrderedDict\n","import os"]},{"cell_type":"markdown","metadata":{"id":"sc_k3JRFQk5n"},"source":["We then define basic network blocks (e.g., conv1x1 or conv3x3 or upsample2d) for setting the basic layer of the monocular depth estimation network.\n"]},{"cell_type":"markdown","metadata":{"id":"zBc_XTg1YNc7"},"source":["For details of FSE module, please refer to HR-Depth: High Resolution Self-Supervised Monocular Depth Estimation(2021 AAAI, Xiaoyang Lyu et al)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"T24DOzBA-Ei4"},"outputs":[],"source":["class Conv1x1(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(Conv1x1, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=False)\n","\n","    def forward(self, x):\n","        return self.conv(x)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"FO1j-bPY9lcD"},"outputs":[],"source":["class Conv3x3(nn.Module):\n","    \"\"\"Layer to pad and convolve input\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels, use_refl=True):\n","        super(Conv3x3, self).__init__()\n","        if use_refl:\n","            self.pad = nn.ReflectionPad2d(1)\n","        else:\n","            self.pad = nn.ZeroPad2d(1)\n","        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n","\n","    def forward(self, x):\n","        out = self.pad(x)\n","        out = self.conv(out)\n","        return out"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"azrcBNgN9oqX"},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    \"\"\"Layer to perform a convolution followed by ELU\n","    \"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        super(ConvBlock, self).__init__()\n","        self.conv = Conv3x3(in_channels, out_channels)\n","        self.nonlin = nn.ELU(inplace=True)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = self.nonlin(out)\n","        return out"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"FJOlRatT9q2N"},"outputs":[],"source":["def upsample(x):\n","    \"\"\"Upsample input tensor by a factor of 2\n","    \"\"\"\n","    return F.interpolate(x, scale_factor=2, mode=\"nearest\")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ENm_WPha9s19"},"outputs":[],"source":["class fSEModule(nn.Module):\n","    def __init__(self, high_feature_channel, low_feature_channels, output_channel=None):\n","        super(fSEModule, self).__init__()\n","        in_channel = high_feature_channel + low_feature_channels\n","        out_channel = high_feature_channel\n","        if output_channel is not None:\n","            out_channel = output_channel\n","        reduction = 16\n","        channel = in_channel\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(channel, channel // reduction, bias=False),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(channel // reduction, channel, bias=False)\n","        )\n","\n","        self.sigmoid = nn.Sigmoid()\n","\n","        self.conv_se = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=1, stride=1)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","    def forward(self, high_features, low_features):\n","        features = [upsample(high_features)]\n","        features += low_features\n","        features = torch.cat(features, 1)\n","\n","        b, c, _, _ = features.size()\n","        y = self.avg_pool(features).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","\n","        y = self.sigmoid(y)\n","        features = features * y.expand_as(features)\n","\n","        return self.relu(self.conv_se(features))"]},{"cell_type":"markdown","metadata":{"id":"wgVzni36Yh7X"},"source":["We define a basic layers to perform the essential functions for networks.\n","After that, we define the ResNet for encoding image information. \n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"u4mQJcQB90BF"},"outputs":[],"source":["class ResNetMultiImageInput(models.ResNet):\n","    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n","        super(ResNetMultiImageInput, self).__init__(block, layers)\n","        self.inplanes = 64\n","        self.conv1 = nn.Conv2d(\n","            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","        self.layer1 = self._make_layer(block, 64, layers[0])\n","        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"rmfsyg9y92fz"},"outputs":[],"source":["def resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n","    \"\"\"Constructs a ResNet model.\n","    Args:\n","        num_layers (int): Number of resnet layers. Must be 18 or 50\n","        pretrained (bool): If True, returns a model pre-trained on ImageNet\n","        num_input_images (int): Number of frames stacked as input\n","    \"\"\"\n","    assert num_layers in [18, 50], \"Can only run with 18 or 50 layer resnet\"\n","    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n","    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n","    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n","\n","    if pretrained:\n","        loaded = model_zoo.load_url(models.resnet.model_urls['resnet{}'.format(num_layers)])\n","        loaded['conv1.weight'] = torch.cat(\n","            [loaded['conv1.weight']] * num_input_images, 1) / num_input_images\n","        model.load_state_dict(loaded)\n","    return model"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"mZysoJBe942i"},"outputs":[],"source":["class ResnetEncoder(nn.Module):\n","    \"\"\"Pytorch module for a resnet encoder\n","    \"\"\"\n","    def __init__(self, num_layers, pretrained, num_input_images=1):\n","        super(ResnetEncoder, self).__init__()\n","\n","        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n","\n","        resnets = {18: models.resnet18,\n","                   34: models.resnet34,\n","                   50: models.resnet50,\n","                   101: models.resnet101,\n","                   152: models.resnet152}\n","\n","        if num_layers not in resnets:\n","            raise ValueError(\"{} is not a valid number of resnet layers\".format(num_layers))\n","\n","        if num_input_images > 1:\n","            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n","        else:\n","            self.encoder = resnets[num_layers](pretrained)\n","\n","        if num_layers > 34:\n","            self.num_ch_enc[1:] *= 4\n","\n","    def forward(self, input_image):\n","        features = []\n","        x = (input_image - 0.45) / 0.225\n","        x = self.encoder.conv1(x)\n","        x = self.encoder.bn1(x)\n","        features.append(self.encoder.relu(x))\n","        features.append(self.encoder.layer1(self.encoder.maxpool(features[-1])))\n","        features.append(self.encoder.layer2(features[-1]))\n","        features.append(self.encoder.layer3(features[-1]))\n","        features.append(self.encoder.layer4(features[-1]))\n","\n","        return features"]},{"cell_type":"markdown","metadata":{"id":"SkZHMupLZgb5"},"source":["After definining the encoder layer, we also define depth decoder layer to decode the depth information."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"Z8P9tmMi-O0e"},"outputs":[],"source":["class HRDepthDecoder(nn.Module):\n","    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, mobile_encoder=False):\n","        super(HRDepthDecoder, self).__init__()\n","\n","        self.num_output_channels = num_output_channels\n","        self.num_ch_enc = num_ch_enc\n","        self.scales = scales\n","        self.mobile_encoder = mobile_encoder\n","        if mobile_encoder:\n","            self.num_ch_dec = np.array([4, 12, 20, 40, 80])\n","        else:\n","            self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n","\n","        self.all_position = [\"01\", \"11\", \"21\", \"31\", \"02\", \"12\", \"22\", \"03\", \"13\", \"04\"]\n","        self.attention_position = [\"31\", \"22\", \"13\", \"04\"]\n","        self.non_attention_position = [\"01\", \"11\", \"21\", \"02\", \"12\", \"03\"]\n","            \n","        self.convs = nn.ModuleDict()\n","        for j in range(5):\n","            for i in range(5 - j):\n","                # upconv 0\n","                num_ch_in = num_ch_enc[i]\n","                if i == 0 and j != 0:\n","                    num_ch_in /= 2\n","                num_ch_out = num_ch_in / 2\n","                self.convs[\"X_{}{}_Conv_0\".format(i, j)] = ConvBlock(num_ch_in, num_ch_out)\n","\n","                # X_04 upconv 1, only add X_04 convolution\n","                if i == 0 and j == 4:\n","                    num_ch_in = num_ch_out\n","                    num_ch_out = self.num_ch_dec[i]\n","                    self.convs[\"X_{}{}_Conv_1\".format(i, j)] = ConvBlock(num_ch_in, num_ch_out)\n","\n","        # declare fSEModule and original module\n","        for index in self.attention_position:\n","            row = int(index[0])\n","            col = int(index[1])\n","            if mobile_encoder:\n","                self.convs[\"X_\" + index + \"_attention\"] = fSEModule(num_ch_enc[row + 1] // 2, self.num_ch_enc[row]\n","                                                                          + self.num_ch_dec[row]*2*(col-1),\n","                                                                         output_channel=self.num_ch_dec[row] * 2)\n","            else:\n","                self.convs[\"X_\" + index + \"_attention\"] = fSEModule(num_ch_enc[row + 1] // 2, self.num_ch_enc[row]\n","                                                                         + self.num_ch_dec[row + 1] * (col - 1))\n","        for index in self.non_attention_position:\n","            row = int(index[0])\n","            col = int(index[1])\n","            if mobile_encoder:\n","                self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)] = ConvBlock(\n","                    self.num_ch_enc[row]+ self.num_ch_enc[row + 1] // 2 +\n","                    self.num_ch_dec[row]*2*(col-1), self.num_ch_dec[row] * 2)\n","            else:\n","                if col == 1:\n","                    self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)] = ConvBlock(num_ch_enc[row + 1] // 2 +\n","                                                                            self.num_ch_enc[row], self.num_ch_dec[row + 1])\n","                else:\n","                    self.convs[\"X_\"+index+\"_downsample\"] = Conv1x1(num_ch_enc[row+1] // 2 + self.num_ch_enc[row]\n","                                                                          + self.num_ch_dec[row+1]*(col-1), self.num_ch_dec[row + 1] * 2)\n","                    self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)] = ConvBlock(self.num_ch_dec[row + 1] * 2, self.num_ch_dec[row + 1])\n","\n","        if self.mobile_encoder:\n","            self.convs[\"dispConvScale0\"] = Conv3x3(4, self.num_output_channels)\n","            self.convs[\"dispConvScale1\"] = Conv3x3(8, self.num_output_channels)\n","            self.convs[\"dispConvScale2\"] = Conv3x3(24, self.num_output_channels)\n","            self.convs[\"dispConvScale3\"] = Conv3x3(40, self.num_output_channels)\n","        else:\n","            for i in range(4):\n","                self.convs[\"dispConvScale{}\".format(i)] = Conv3x3(self.num_ch_dec[i], self.num_output_channels)\n","\n","        self.decoder = nn.ModuleList(list(self.convs.values()))\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def nestConv(self, conv, high_feature, low_features):\n","        conv_0 = conv[0]\n","        conv_1 = conv[1]\n","        assert isinstance(low_features, list)\n","        high_features = [upsample(conv_0(high_feature))]\n","        for feature in low_features:\n","            high_features.append(feature)\n","        high_features = torch.cat(high_features, 1)\n","        if len(conv) == 3:\n","            high_features = conv[2](high_features)\n","        return conv_1(high_features)\n","\n","    def forward(self, input_features):\n","        outputs = {}\n","        features = {}\n","        for i in range(5):\n","            features[\"X_{}0\".format(i)] = input_features[i]\n","        # Network architecture\n","        for index in self.all_position:\n","            row = int(index[0])\n","            col = int(index[1])\n","            low_features = []\n","            for i in range(col):\n","                low_features.append(features[\"X_{}{}\".format(row, i)])\n","            # add fSE block to decoder\n","            if index in self.attention_position:\n","                features[\"X_\"+index] = self.convs[\"X_\" + index + \"_attention\"](\n","                    self.convs[\"X_{}{}_Conv_0\".format(row+1, col-1)](features[\"X_{}{}\".format(row+1, col-1)]), low_features)\n","            elif index in self.non_attention_position:\n","                conv = [self.convs[\"X_{}{}_Conv_0\".format(row + 1, col - 1)],\n","                        self.convs[\"X_{}{}_Conv_1\".format(row + 1, col - 1)]]\n","                if col != 1 and not self.mobile_encoder:\n","                    conv.append(self.convs[\"X_\" + index + \"_downsample\"])\n","                features[\"X_\" + index] = self.nestConv(conv, features[\"X_{}{}\".format(row+1, col-1)], low_features)\n","\n","        x = features[\"X_04\"]\n","        x = self.convs[\"X_04_Conv_0\"](x)\n","        x = self.convs[\"X_04_Conv_1\"](upsample(x))\n","        outputs[(\"disparity\", \"Scale0\")] = self.sigmoid(self.convs[\"dispConvScale0\"](x))\n","        outputs[(\"disparity\", \"Scale1\")] = self.sigmoid(self.convs[\"dispConvScale1\"](features[\"X_04\"]))\n","        outputs[(\"disparity\", \"Scale2\")] = self.sigmoid(self.convs[\"dispConvScale2\"](features[\"X_13\"]))\n","        outputs[(\"disparity\", \"Scale3\")] = self.sigmoid(self.convs[\"dispConvScale3\"](features[\"X_22\"]))\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"12kgHnaJZrk6"},"source":["We have defined the basic layer for every network. Load the checkpoint and extract the result."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1669876519518,"user":{"displayName":"윤성훈","userId":"03235101598759790024"},"user_tz":-540},"id":"ro3m-eew-RFG","outputId":"36dec423-9a44-4609-be23-eccb0d319e80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test image height is: 384\n","Test image width is: 1280\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["depth_encoder = ResnetEncoder(18, False)\n","depth_decoder = HRDepthDecoder(depth_encoder.num_ch_enc)\n","\n","depth_encoder_path = 'ckpts/encoder.pth'\n","depth_decoder_path = 'ckpts/depth.pth'\n","\n","encoder_dict = torch.load(depth_encoder_path)\n","img_height = encoder_dict[\"height\"]\n","img_width = encoder_dict[\"width\"]\n","print(\"Test image height is:\", img_height)\n","print(\"Test image width is:\", img_width)\n","load_dict = {k: v for k, v in encoder_dict.items() if k in depth_encoder.state_dict()}\n","\n","decoder_dict = torch.load(depth_decoder_path)\n","\n","depth_encoder.load_state_dict(load_dict)\n","depth_decoder.load_state_dict(decoder_dict)"]},{"cell_type":"markdown","metadata":{},"source":["Next, we define object detection network"]},{"cell_type":"markdown","metadata":{},"source":["# Object Detection"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["import argparse\n","import time\n","from pathlib import Path\n","import math\n","import torchvision\n","import glob\n","import cv2\n","import torch\n","import torch.backends.cudnn as cudnn\n","from numpy import random\n","import numpy as np\n","import torch.nn as nn\n","import os\n","\n","def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n","    # Rescale coords (xyxy) from img1_shape to img0_shape\n","    if ratio_pad is None:  # calculate from img0_shape\n","        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n","        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n","    else:\n","        gain = ratio_pad[0][0]\n","        pad = ratio_pad[1]\n","\n","    coords[:, [0, 2]] -= pad[0]  # x padding\n","    coords[:, [1, 3]] -= pad[1]  # y padding\n","    coords[:, :4] /= gain\n","    coords[:, 0].clamp_(0, img0_shape[1])  # x1\n","    coords[:, 1].clamp_(0, img0_shape[0])  # y1\n","    coords[:, 2].clamp_(0, img0_shape[1])  # x2\n","    coords[:, 3].clamp_(0, img0_shape[0])  # y2\n","    return coords\n","\n","def xyxy2xywh(x):\n","    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n","    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n","    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n","    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n","    y[:, 2] = x[:, 2] - x[:, 0]  # width\n","    y[:, 3] = x[:, 3] - x[:, 1]  # height\n","    return y\n","\n","def xywh2xyxy(x):\n","    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n","    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n","    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n","    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n","    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n","    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n","    return y\n","\n","def box_iou(box1, box2):\n","    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n","    def box_area(box):\n","        # box = 4xn\n","        return (box[2] - box[0]) * (box[3] - box[1])\n","\n","    area1 = box_area(box1.T)\n","    area2 = box_area(box2.T)\n","\n","    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n","    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n","    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n","\n","def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, agnostic=False, multi_label=False,\n","                        labels=()):\n","    \n","    nc = prediction.shape[2] - 5  # number of classes\n","    xc = prediction[..., 4] > conf_thres  # candidates\n","\n","    # Settings\n","    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n","    max_det = 300  # maximum number of detections per image\n","    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n","    time_limit = 10.0  # seconds to quit after\n","    redundant = True  # require redundant detections\n","    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n","    merge = False  # use merge-NMS\n","\n","    t = time.time()\n","    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n","    for xi, x in enumerate(prediction):  # image index, image inference\n","        # Apply constraints\n","        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n","        x = x[xc[xi]]  # confidence\n","\n","        # Cat apriori labels if autolabelling\n","        if labels and len(labels[xi]):\n","            l = labels[xi]\n","            v = torch.zeros((len(l), nc + 5), device=x.device)\n","            v[:, :4] = l[:, 1:5]  # box\n","            v[:, 4] = 1.0  # conf\n","            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n","            x = torch.cat((x, v), 0)\n","\n","        # If none remain process next image\n","        if not x.shape[0]:\n","            continue\n","\n","        # Compute conf\n","        if nc == 1:\n","            x[:, 5:] = x[:, 4:5] # for models with one class, cls_loss is 0 and cls_conf is always 0.5,\n","                                 # so there is no need to multiplicate.\n","        else:\n","            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n","\n","        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n","        box = xywh2xyxy(x[:, :4])\n","\n","        # Detections matrix nx6 (xyxy, conf, cls)\n","        if multi_label:\n","            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n","            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n","        else:  # best class only\n","            conf, j = x[:, 5:].max(1, keepdim=True)\n","            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n","\n","        # Check shape\n","        n = x.shape[0]  # number of boxes\n","        if not n:  # no boxes\n","            continue\n","        elif n > max_nms:  # excess boxes\n","            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n","\n","        # Batched NMS\n","        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n","        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n","        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n","        if i.shape[0] > max_det:  # limit detections\n","            i = i[:max_det]\n","        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n","            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n","            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n","            weights = iou * scores[None]  # box weights\n","            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n","            if redundant:\n","                i = i[iou.sum(1) > 1]  # require redundancy\n","\n","        output[xi] = x[i]\n","        if (time.time() - t) > time_limit:\n","            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n","            break  # time limit exceeded\n","\n","    return output\n","\n","def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n","    # Resize and pad image while meeting stride-multiple constraints\n","    shape = img.shape[:2]  # current shape [height, width]\n","    if isinstance(new_shape, int):\n","        new_shape = (new_shape, new_shape)\n","\n","    # Scale ratio (new / old)\n","    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n","\n","    # Compute padding\n","    ratio = r, r  # width, height ratios\n","    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n","    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n","    dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n","    \n","    dw /= 2  # divide padding into 2 sides\n","    dh /= 2\n","\n","    if shape[::-1] != new_unpad:  # resize\n","        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n","    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n","    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n","    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n","    return img, ratio, (dw, dh)\n","\n","class LoadImages:  # for inference\n","    def __init__(self, path, img_size=640, stride=32):\n","        p = str(Path(path).absolute())  # os-agnostic absolute path\n","        \n","        if os.path.isdir(p):\n","            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\n","        elif os.path.isfile(p):\n","            files = [p]  # files\n","        images = [x for x in files]\n","        ni = len(images)\n","\n","        self.img_size = img_size\n","        self.stride = stride\n","        self.files = images\n","        self.nf = ni  # number of files\n","        self.mode = 'image'\n","        self.cap = None\n","        \n","\n","    def __iter__(self):\n","        self.count = 0\n","        return self\n","\n","    def __next__(self):\n","        if self.count == self.nf:\n","            raise StopIteration\n","        path = self.files[self.count]\n","        \n","        self.count += 1\n","        img0 = cv2.imread(path)  # BGR\n","        assert img0 is not None, 'Image Not Found ' + path\n","\n","        # Padded resize\n","        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n","\n","        # Convert\n","        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n","        img = np.ascontiguousarray(img)\n","\n","        return path, img, img0, self.cap\n","\n","    def __len__(self):\n","        return self.nf  # number of files\n","    \n","class TracedModel(nn.Module):\n","\n","    def __init__(self, model=None, device=None, img_size=(640,640)): \n","        super(TracedModel, self).__init__()\n","        \n","        print(\" Convert model to Traced-model... \") \n","        self.stride = model.stride\n","        self.names = model.names\n","        self.model = model\n","\n","        self.model.to('cpu')\n","        self.model.eval()\n","\n","        self.detect_layer = self.model.model[-1]\n","        self.model.traced = True\n","        \n","        rand_example = torch.rand(1, 3, img_size, img_size)\n","        \n","        traced_script_module = torch.jit.trace(self.model, rand_example, strict=False)\n","        #traced_script_module = torch.jit.script(self.model)\n","        self.model = traced_script_module\n","        self.model.to(device)\n","        self.detect_layer.to(device)\n","\n","    def forward(self, x, augment=False, profile=False):\n","        out = self.model(x)\n","        out = self.detect_layer(out)\n","        return out\n","\n","def autopad(k, p=None):  # kernel, padding\n","    # Pad to 'same'\n","    if p is None:\n","        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n","    return p    \n","    \n","class Conv(nn.Module):\n","    # Standard convolution\n","    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n","        super(Conv, self).__init__()\n","        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n","        self.bn = nn.BatchNorm2d(c2)\n","        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n","\n","    def forward(self, x):\n","        return self.act(self.bn(self.conv(x)))\n","\n","    def fuseforward(self, x):\n","        return self.act(self.conv(x))\n","\n","def crop_bbox(x, img):\n","    \n","    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n","    cropped_img = img[c1[1]:c2[1], c1[0]:c2[0]]\n","    return cropped_img\n","\n","class Ensemble(nn.ModuleList):\n","    # Ensemble of models\n","    def __init__(self):\n","        super(Ensemble, self).__init__()\n","\n","    def forward(self, x, augment=False):\n","        y = []\n","        for module in self:\n","            y.append(module(x, augment)[0])\n","        # y = torch.stack(y).max(0)[0]  # max ensemble\n","        # y = torch.stack(y).mean(0)  # mean ensemble\n","        y = torch.cat(y, 1)  # nms ensemble\n","        return y, None  # inference, train output    \n","    \n","    "]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Fusing layers... \n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","RepConv.fuse_repvgg_block\n","IDetect.fuse\n"," Convert model to Traced-model... \n"]}],"source":["weights = 'ckpts/yolov7_cityscapes.pt'\n","\n","device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# Load model\n","model = Ensemble()\n","sys.path.insert(0, './model')\n","ckpt = torch.load(weights, device)  # load\n","model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model\n","\n","# Compatibility updates\n","for m in model.modules():\n","    if type(m) in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:\n","        m.inplace = True \n","    elif type(m) is nn.Upsample:\n","        m.recompute_scale_factor = None  \n","    elif type(m) is Conv:\n","        m._non_persistent_buffers_set = set() \n","model=model[-1]  \n","detection_img_size = 640 \n","stride = int(model.stride.max())  # model stride\n","imgsz = math.ceil(detection_img_size / int(stride)) * int(stride)\n","\n","model = TracedModel(model, device, detection_img_size)\n","model.half()  # to FP16\n","\n","names = model.module.names if hasattr(model, 'module') else model.names\n","\n","names = ['person', 'car', 'truck', 'rider',\n","            'motorcycle', 'bicycle', 'bus', 'train']\n","colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]"]},{"cell_type":"markdown","metadata":{"id":"sI_6KYTYsjeV"},"source":["#Get Semantic segmentation results and mono depth estimation results"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"elapsed":3638,"status":"error","timestamp":1669881605497,"user":{"displayName":"윤성훈","userId":"03235101598759790024"},"user_tz":-540},"id":"hSBrc9k7GnCE","outputId":"423c9ff1-f456-43bb-fce4-194b3efed4db"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 300/300 [00:16<00:00, 18.55it/s]\n"]}],"source":["import cv2\n","import os\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import glob\n","# import system\n","from tqdm import tqdm\n","\n","MEAN = [0.45734706, 0.43338275, 0.40058118]\n","STD = [0.23965294, 0.23532275, 0.2398498]\n","H_org = 1024\n","W_org = 2048\n","\n","toTensor = transforms.ToTensor()\n","normTensor = transforms.Normalize(MEAN,STD)\n","\n","root_dir = \".\" \n","\n","result_dir = os.path.join(root_dir,\"results\")\n","\n","os.makedirs(result_dir,exist_ok=True)\n","\n","seg_dir = os.path.join(result_dir,\"segmentation\")\n","os.makedirs(seg_dir,exist_ok=True)\n","\n","seg_color_dir = os.path.join(seg_dir,\"color\")\n","seg_pred_dir = os.path.join(seg_dir,\"pred\")\n","os.makedirs(seg_color_dir,exist_ok=True)\n","os.makedirs(seg_pred_dir,exist_ok=True)\n","\n","depth_pred_dir = os.path.join(result_dir,\"depth\")\n","os.makedirs(depth_pred_dir,exist_ok=True)\n","\n","images = glob.glob(os.path.join(root_dir,'student_dataset/train/current_image/cri_0/*.png'))\n","\n","for image in tqdm(images):\n","  name = os.path.basename(image)\n","  image = cv2.cvtColor(cv2.imread(image), cv2.COLOR_RGB2BGR)\n","\n","  imageT = normTensor(toTensor(image))\n","\n","\n","  ########################################\n","  ###########Your Implementation##########\n","  ########################################\n","\n","  #save the sem-seg pred result to \"seg_pred_dir\"\n","  #save the sem-seg colormap result to \"seg_color_dir\"\n","  #save the mono-depth pred result to \"depth_pred_dir\""]},{"cell_type":"markdown","metadata":{},"source":["#Get Object detection results"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["source = os.path.join(root_dir,'student_dataset/train/current_image/cri_0')\n","dataset = LoadImages(source, img_size=imgsz, stride=stride)\n","root_dir = \".\" \n","result_dir = os.path.join(root_dir,\"results\")\n","object_detection_pred_dir = os.path.join(result_dir,\"object_detection\")\n","os.makedirs(object_detection_pred_dir,exist_ok=True)\n","\n","for path, img, im0s, vid_cap in dataset:\n","    img = torch.from_numpy(img).to(device)\n","    img = img.half()  # uint8 to fp16/32\n","    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n","    if img.ndimension() == 3:\n","        img = img.unsqueeze(0)\n","    # Inference\n","    with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n","        pred = model(img)[0]\n","        # Apply NMS\n","        pred = non_max_suppression(pred)\n","        # Process detections\n","        for i, det in enumerate(pred): \n","            \n","            #########################################\n","            ### Example Code for Object Detection ###\n","            #########################################\n","            \n","            p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n","            p = Path(p)  # to Path\n","            save_path = str(object_detection_pred_dir +'/'+ str(p.name)) \n","\n","            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n","            if len(det):\n","                # Rescale boxes from img_size to original_image size\n","                det[:, :4] = scale_coords(\n","                    img.shape[2:], det[:, :4], im0.shape).round()\n","                \n","                # Write results\n","                for *xyxy, conf, cls in reversed(det):\n","                    pass\n","                ########################################\n","                ###########Your Implementation##########\n","                ########################################\n","\n","                # save the object detection pred result to \"od_pred_dir\"\n","                # use xyxy and cls to identify where and which object is detected\n","            "]},{"cell_type":"markdown","metadata":{"id":"ZFBjBffPqoRA"},"source":["# CRI estimation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGJqa5pMqs7l"},"outputs":[],"source":["  ########################################\n","  ###########Your Implementation##########\n","  ########################################"]},{"cell_type":"markdown","metadata":{"id":"vjWyk4aMsV1T"},"source":["#Sanity Check (validation set)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sm_lDAe_saOZ"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dusseldorf_000103_000020_leftImg8bit.png': 0, 'jena_000107_000020_leftImg8bit.png': 0, 'leverkusen_000051_000020_leftImg8bit.png': 0, 'stuttgart_000071_000020_leftImg8bit.png': 0, 'cologne_000039_000020_leftImg8bit.png': 0, 'munich_000246_000020_leftImg8bit.png': 0, 'bremen_000203_000020_leftImg8bit.png': 0, 'tubingen_000016_000020_leftImg8bit.png': 0, 'munich_000375_000020_leftImg8bit.png': 0, 'leverkusen_000028_000020_leftImg8bit.png': 0, 'stuttgart_000078_000020_leftImg8bit.png': 0, 'dusseldorf_000130_000020_leftImg8bit.png': 0, 'munich_000384_000020_leftImg8bit.png': 0, 'leverkusen_000036_000020_leftImg8bit.png': 0, 'leverkusen_000007_000020_leftImg8bit.png': 0, 'bremen_000184_000020_leftImg8bit.png': 0, 'leverkusen_000030_000020_leftImg8bit.png': 0, 'bremen_000206_000020_leftImg8bit.png': 0, 'stuttgart_000074_000020_leftImg8bit.png': 0, 'dusseldorf_000127_000020_leftImg8bit.png': 0, 'tubingen_000017_000020_leftImg8bit.png': 0, 'leverkusen_000043_000020_leftImg8bit.png': 0, 'tubingen_000009_000020_leftImg8bit.png': 0, 'jena_000113_000020_leftImg8bit.png': 0, 'cologne_000016_000020_leftImg8bit.png': 0, 'jena_000106_000020_leftImg8bit.png': 0, 'cologne_000057_000020_leftImg8bit.png': 0, 'cologne_000029_000020_leftImg8bit.png': 0, 'cologne_000048_000020_leftImg8bit.png': 0, 'leverkusen_000042_000020_leftImg8bit.png': 0, 'dusseldorf_000125_000020_leftImg8bit.png': 0, 'leverkusen_000045_000020_leftImg8bit.png': 0, 'leverkusen_000046_000020_leftImg8bit.png': 0, 'cologne_000063_000020_leftImg8bit.png': 0, 'munich_000253_000020_leftImg8bit.png': 0, 'cologne_000050_000020_leftImg8bit.png': 0, 'dusseldorf_000119_000020_leftImg8bit.png': 0, 'munich_000345_000020_leftImg8bit.png': 0, 'dusseldorf_000106_000020_leftImg8bit.png': 0, 'munich_000245_000020_leftImg8bit.png': 0, 'munich_000370_000020_leftImg8bit.png': 0, 'munich_000362_000020_leftImg8bit.png': 0, 'cologne_000064_000020_leftImg8bit.png': 0, 'leverkusen_000027_000020_leftImg8bit.png': 0, 'munich_000376_000020_leftImg8bit.png': 0, 'cologne_000045_000020_leftImg8bit.png': 0, 'leverkusen_000017_000020_leftImg8bit.png': 0, 'munich_000367_000020_leftImg8bit.png': 0, 'leverkusen_000008_000020_leftImg8bit.png': 0, 'bremen_000193_000020_leftImg8bit.png': 0, 'jena_000118_000020_leftImg8bit.png': 0, 'leverkusen_000000_000020_leftImg8bit.png': 0, 'dusseldorf_000116_000020_leftImg8bit.png': 0, 'stuttgart_000064_000020_leftImg8bit.png': 0, 'dusseldorf_000123_000020_leftImg8bit.png': 0, 'munich_000382_000020_leftImg8bit.png': 0, 'munich_000354_000020_leftImg8bit.png': 0, 'munich_000377_000020_leftImg8bit.png': 0, 'dusseldorf_000128_000020_leftImg8bit.png': 0, 'bremen_000209_000020_leftImg8bit.png': 0, 'tubingen_000049_000020_leftImg8bit.png': 0, 'tubingen_000086_000020_leftImg8bit.png': 0, 'bremen_000144_000020_leftImg8bit.png': 0, 'bonn_000033_000020_leftImg8bit.png': 0, 'erfurt_000042_000020_leftImg8bit.png': 0, 'munich_000001_000020_leftImg8bit.png': 0, 'dusseldorf_000181_000020_leftImg8bit.png': 0, 'aachen_000131_000020_leftImg8bit.png': 0, 'munich_000148_000020_leftImg8bit.png': 0, 'dusseldorf_000104_000020_leftImg8bit.png': 0, 'munich_000144_000020_leftImg8bit.png': 0, 'dusseldorf_000065_000020_leftImg8bit.png': 0, 'stuttgart_000187_000020_leftImg8bit.png': 0, 'ulm_000059_000020_leftImg8bit.png': 0, 'mainz_000002_000062_leftImg8bit.png': 0, 'stuttgart_000152_000020_leftImg8bit.png': 0, 'tubingen_000036_000020_leftImg8bit.png': 0, 'dusseldorf_000152_000020_leftImg8bit.png': 0, 'munich_000104_000020_leftImg8bit.png': 0, 'erfurt_000002_000020_leftImg8bit.png': 0, 'bremen_000210_000020_leftImg8bit.png': 0, 'munich_000134_000020_leftImg8bit.png': 0, 'dusseldorf_000084_000020_leftImg8bit.png': 0, 'darmstadt_000067_000020_leftImg8bit.png': 0, 'bremen_000201_000020_leftImg8bit.png': 0, 'munich_000052_000020_leftImg8bit.png': 0, 'darmstadt_000061_000020_leftImg8bit.png': 0, 'dusseldorf_000172_000020_leftImg8bit.png': 0, 'tubingen_000063_000020_leftImg8bit.png': 0, 'jena_000115_000020_leftImg8bit.png': 0, 'weimar_000035_000020_leftImg8bit.png': 0, 'tubingen_000068_000020_leftImg8bit.png': 0, 'bremen_000110_000020_leftImg8bit.png': 0, 'stuttgart_000163_000020_leftImg8bit.png': 0, 'tubingen_000065_000020_leftImg8bit.png': 0, 'weimar_000025_000020_leftImg8bit.png': 0, 'munich_000122_000020_leftImg8bit.png': 0, 'weimar_000026_000020_leftImg8bit.png': 0, 'tubingen_000043_000020_leftImg8bit.png': 0, 'leverkusen_000039_000020_leftImg8bit.png': 0, 'jena_000051_000020_leftImg8bit.png': 0, 'weimar_000042_000020_leftImg8bit.png': 0, 'munich_000078_000020_leftImg8bit.png': 0, 'erfurt_000008_000020_leftImg8bit.png': 0, 'jena_000078_000020_leftImg8bit.png': 0, 'munich_000168_000020_leftImg8bit.png': 0, 'aachen_000140_000020_leftImg8bit.png': 0, 'bremen_000109_000020_leftImg8bit.png': 0, 'stuttgart_000107_000020_leftImg8bit.png': 0, 'munster_000166_000020_leftImg8bit.png': 0, 'bremen_000071_000020_leftImg8bit.png': 0, 'zurich_000029_000020_leftImg8bit.png': 0, 'darmstadt_000035_000020_leftImg8bit.png': 0, 'tubingen_000008_000020_leftImg8bit.png': 0, 'tubingen_000037_000020_leftImg8bit.png': 0, 'darmstadt_000059_000020_leftImg8bit.png': 0, 'zurich_000003_000020_leftImg8bit.png': 0, 'munster_000151_000020_leftImg8bit.png': 0, 'bremen_000054_000020_leftImg8bit.png': 0, 'darmstadt_000053_000020_leftImg8bit.png': 0, 'bremen_000052_000020_leftImg8bit.png': 0, 'stuttgart_000106_000020_leftImg8bit.png': 0, 'dusseldorf_000040_000020_leftImg8bit.png': 0, 'stuttgart_000063_000020_leftImg8bit.png': 0, 'zurich_000057_000020_leftImg8bit.png': 0, 'zurich_000021_000020_leftImg8bit.png': 0, 'darmstadt_000051_000020_leftImg8bit.png': 0, 'stuttgart_000158_000020_leftImg8bit.png': 0, 'tubingen_000007_000020_leftImg8bit.png': 0, 'tubingen_000026_000020_leftImg8bit.png': 0, 'stuttgart_000124_000020_leftImg8bit.png': 0, 'darmstadt_000065_000020_leftImg8bit.png': 0, 'tubingen_000031_000020_leftImg8bit.png': 0, 'lindau_000037_000020_leftImg8bit.png': 0, 'cologne_000051_000020_leftImg8bit.png': 0, 'cologne_000018_000020_leftImg8bit.png': 0, 'darmstadt_000022_000020_leftImg8bit.png': 0, 'cologne_000020_000020_leftImg8bit.png': 0, 'tubingen_000050_000020_leftImg8bit.png': 0, 'darmstadt_000034_000020_leftImg8bit.png': 0, 'darmstadt_000066_000020_leftImg8bit.png': 0, 'tubingen_000057_000020_leftImg8bit.png': 0, 'stuttgart_000168_000020_leftImg8bit.png': 0, 'darmstadt_000070_000020_leftImg8bit.png': 0, 'darmstadt_000069_000020_leftImg8bit.png': 0, 'munich_000042_000020_leftImg8bit.png': 0, 'tubingen_000047_000020_leftImg8bit.png': 0, 'cologne_000075_000020_leftImg8bit.png': 0, 'cologne_000100_000020_leftImg8bit.png': 0, 'munich_000079_000020_leftImg8bit.png': 0, 'lindau_000025_000020_leftImg8bit.png': 0, 'leverkusen_000044_000020_leftImg8bit.png': 0, 'darmstadt_000007_000020_leftImg8bit.png': 0, 'cologne_000134_000020_leftImg8bit.png': 0, 'cologne_000106_000020_leftImg8bit.png': 0, 'cologne_000086_000020_leftImg8bit.png': 0, 'stuttgart_000022_000020_leftImg8bit.png': 0, 'cologne_000121_000020_leftImg8bit.png': 0, 'munster_000102_000020_leftImg8bit.png': 0, 'munster_000026_000020_leftImg8bit.png': 0, 'stuttgart_000017_000020_leftImg8bit.png': 0, 'darmstadt_000003_000020_leftImg8bit.png': 0, 'cologne_000056_000020_leftImg8bit.png': 0, 'cologne_000033_000020_leftImg8bit.png': 0, 'cologne_000135_000020_leftImg8bit.png': 0, 'munster_000141_000020_leftImg8bit.png': 0, 'stuttgart_000110_000020_leftImg8bit.png': 0, 'munster_000046_000020_leftImg8bit.png': 0, 'munster_000061_000020_leftImg8bit.png': 0, 'cologne_000128_000020_leftImg8bit.png': 0, 'stuttgart_000012_000020_leftImg8bit.png': 0, 'cologne_000108_000020_leftImg8bit.png': 0}\n"]},{"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'keys'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[28], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(valGT)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Ensure keys are identical\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m keys_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(valGT\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mvalPred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeys match validation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeys_match\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate accuracy (assuming dummy_GT has all values 0)\u001b[39;00m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'keys'"]}],"source":["valGT = np.load(\"val_sanity.npy\",allow_pickle=True).item()\n","\n","valPred = None #Insert YOUR PREDITION \"Dictionary\" File\n","print(valGT)\n","\n","# Ensure keys are identical\n","keys_match = set(valGT.keys()) == set(valPred.keys())\n","print(f\"Keys match validation: {keys_match}\")\n","# Calculate accuracy (assuming dummy_GT has all values 0)\n","assert sum(1 for key in valGT if valGT[key] == valPred[key]), \"Value Type Error\"\n","print(f\"Total validation: True\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPq4laaVEZdmwRPp0XcbCk2","machine_shape":"hm","provenance":[{"file_id":"1aRRLq8ieF3qHw3i-XtPX7KLQBMwXyn3s","timestamp":1669880344909}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
